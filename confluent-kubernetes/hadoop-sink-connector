kubectl exec -it -n confluent connect-0 -- curl -X POST -H "Content-Type: application/json" --data '
  {"name": "hdfs3-sink-wekan-sett",
   "config": {
     "connector.class": "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
     "tasks.max": "1",
     "topics": "confluent.connect-configs",
     "hdfs.url": "hdfs://hadoop-hadoop-hdfs-nn.hadoop.svc.cluster.local:9000",
     "flush.size": "3"
   }}' http://connect-0.connect:8083/connectors -w "\n"

kubectl exec -it -n confluent connect-0 -- curl -X POST -H "Content-Type: application/json" --data '
  {"name": "hdfs3-sink-wekan-setting",
   "config": {
     "connector.class": "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
     "tasks.max": "1",
     "topics": "wekan.wekan.settings",
     "hdfs.url": "hdfs://hadoop-hadoop-hdfs-nn.hadoop.svc.cluster.local:9000",
     "flush.size": "3",
     "key.converter":"org.apache.kafka.connect.json.JsonConverter",
     "key.converter.schemas.enable":false,
     "value.converter":"org.apache.kafka.connect.json.JsonConverter",
     "value.converter.schemas.enable":false,
     "confluent.topic.bootstrap.servers": "kafka.confluent.svc.cluster.local:9092",
     "confluent.topic.replication.factor": "1"
   }}' http://connect-0.connect:8083/connectors -w "\n"

kubectl exec -it -n confluent connect-0 -- curl -X POST -H "Content-Type: application/json" --data '
  {"name": "mongo-sink-wekan-accountSettings",
   "config": {
     "tasks.max":"1",
     "connector.class":"com.mongodb.kafka.connect.MongoSinkConnector",
     "topics":"accountSettings.wekan.accountSettings",
     "key.converter":"org.apache.kafka.connect.json.JsonConverter",
     "key.converter.schemas.enable":false,
     "value.converter":"org.apache.kafka.connect.json.JsonConverter",
     "value.converter.schemas.enable":false,
     "publish.full.document.only":true,
     "connection.uri":"mongodb://root:pass@mongodb-headless.wekan-project.svc.cluster.local:27017",
     "topic.prefix":"accountSettings",
     "database":"wekan-sink",
     "collection":"accountSettings",
     "poll.await.time.ms":"500",
     "poll.max.batch.size":"1000",
     "document.id.strategy.overwrite.existing":true,
     "copy.existing":true
   }}' http://connect-0.connect:8083/connectors -w "\n"




















{
  "name": "hdfs3-sink",
  "config": {
    "connector.class": "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
    "tasks.max": "1",
    "topics": "test_hdfs",
    "hdfs.url": "hdfs://localhost:9000",
    "flush.size": "3",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url":"http://localhost:8081",
    "confluent.topic.bootstrap.servers": "localhost:9092",
    "confluent.topic.replication.factor": "1"
  }
}

# list files in partition called is_customer=true
hadoop fs -ls /topics/parquet_field_hdfs/is_customer=true
# list files in partition 0
hadoop fs -ls /topics/test_hdfs/partition=0


# substitute "<namenode>" for the HDFS name node hostname
hadoop jar parquet-tools-1.9.0.jar cat --json / hdfs://<namenode>/topics/parquet_field_hdfs/is_customer=true/parquet_field_hdfs+0+0000000000+0000000002.parquet

# Copy file out
hadoop fs -copyToLocal /topics/parquet_field_hdfs/is_customer=true/parquet_field_hdfs+0+0000000000+0000000002.parquet / /tmp/parquet_field_hdfs+0+0000000000+0000000002.parquet